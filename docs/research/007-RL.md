# Reinforcement Learning (RL)
## Introduction
Reinforcement learning is a type of machine learning where an agent learns to make decisions by interacting with an environment. The agent takes actions to maximize cumulative rewards over time, receiving feedback in the form of rewards or penalties. Unlike supervised learning, it doesn't rely on labeled data but learns through trial and error. Reinforcement learning is commonly used in robotics, game playing, and autonomous systems.  
  
Methods that can be used for this purpose:
- PPO
- SAC
- DQN
  
## Methods
### Proximal Policy Optimization (PPO)
A policy gradient method that balances performance and stability by using a clipped objective function. It improves learning efficiency and is widely used in continuous and discrete action spaces.  
  
✅ PROS:  
- **Stable and robust:** Works well in real-world robotics and autonomous driving.  
- **Policy-based:** Handles continuous action spaces smoothly.  
- **Efficient:** Uses a clipped objective function to prevent large policy updates, leading to stable learning.  
- **Parallelizable:** Works well in distributed training environments.  
  
❌ CONS:  
- **Less sample efficient:** Requires more interactions with the environment compared to off-policy methods.  
- **Still needs hyperparameter tuning:** Finding the right clipping parameter is crucial for stability.  
- Can be slower to converge than TD3 or SAC.

### Soft Actor-Critic (SAC)
An off-policy actor-critic algorithm that maximizes both reward and entropy, encouraging exploration. It's stable and efficient in continuous action spaces, making it ideal for robotics and control tasks.  
  
✅ PROS:  
- **Explores more effectively:** Uses entropy regularization, making it good at balancing exploration and exploitation.  
- **Stable and robust learning:** Works well for continuous action spaces.  
- **Off-policy:** More sample-efficient than PPO.  
- Performs well in complex, high-dimensional tasks.  
  
❌ CONS:  
- **Computationally expensive:** Requires learning two Q-functions and an actor.  
- More complex implementation than PPO or TD3.  
- Entropy tuning can be tricky, affecting performance.  
  
### Deep Q-Network (DQN)
A value-based method that combines Q-learning with deep neural networks to handle high-dimensional state spaces, typically in discrete action environments like Atari games.  
  
✅ PROS:  
- **Simple to implement:** Works well with discrete action spaces.  
- **Sample efficient:** Uses experience replay and target networks.  
- Good for learning basic driving behaviors.  
- Less computationally expensive compared to PPO, SAC, or TD3.  
  
❌ CONS:  
- **Only works with discrete actions:** Not suitable for smooth control in autonomous driving.  
- **Struggles with continuous environments:** Needs discretization, which can lead to suboptimal behavior.  
- **Prone to overestimation bias:** Can make learning unstable.  
- Less effective for high-dimensional tasks.  

___