<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Hotwheels-Lane_Detection: Autonomous Driving Method</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(function() { init_search(); });
/* @license-end */
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Hotwheels-Lane_Detection<span id="projectnumber">&#160;1.2</span>
   </div>
   <div id="projectbrief">Creation of Lane_Detection for SEA:ME project.</div>
  </td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.13.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('md_ADR_2002-AutonomousDrivingMethod.html',''); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">Autonomous Driving Method</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="autotoc_md4"></a></p>
<h1><a class="anchor" id="autotoc_md5"></a>
Context</h1>
<p>Being able to detect lanes correctly is very important in the process of developing a self driving car. However, the real challenge is the decision making that comes after those detections. In order to implement an autonomous driving algorithm with reinforcement learning, the team had to do a small research to determine the best approach to use in our project. To achieve this we gathered the 4 most used methods and created a pros/cons list to help us decide.</p>
<h2><a class="anchor" id="autotoc_md6"></a>
Results</h2>
<h3><a class="anchor" id="autotoc_md7"></a>
1. Proximal Policy Optimization (PPO)</h3>
<p>✅ PROS: <br  />
</p><ul>
<li><b>Stable and robust:</b> Works well in real-world robotics and autonomous driving. <br  />
</li>
<li><b>Policy-based:</b> Handles continuous action spaces smoothly. <br  />
</li>
<li><b>Efficient:</b> Uses a clipped objective function to prevent large policy updates, leading to stable learning. <br  />
</li>
<li><b>Parallelizable:</b> Works well in distributed training environments. <br  />
</li>
</ul>
<p>❌ CONS: <br  />
</p><ul>
<li><b>Less sample efficient:</b> Requires more interactions with the environment compared to off-policy methods. <br  />
</li>
<li><b>Still needs hyperparameter tuning:</b> Finding the right clipping parameter is crucial for stability. <br  />
</li>
<li>Can be slower to converge than TD3 or SAC.</li>
</ul>
<h3><a class="anchor" id="autotoc_md8"></a>
2. Soft Actor-Critic (SAC)</h3>
<p>✅ PROS: <br  />
</p><ul>
<li><b>Explores more effectively:</b> Uses entropy regularization, making it good at balancing exploration and exploitation. <br  />
</li>
<li><b>Stable and robust learning:</b> Works well for continuous action spaces. <br  />
</li>
<li><b>Off-policy:</b> More sample-efficient than PPO. <br  />
</li>
<li>Performs well in complex, high-dimensional tasks. <br  />
</li>
</ul>
<p>❌ CONS: <br  />
</p><ul>
<li><b>Computationally expensive:</b> Requires learning two Q-functions and an actor. <br  />
</li>
<li>More complex implementation than PPO or TD3. <br  />
</li>
<li>Entropy tuning can be tricky, affecting performance. <br  />
</li>
</ul>
<h3><a class="anchor" id="autotoc_md9"></a>
3. Twin Delayed Deep Deterministic Policy Gradient (TD3)</h3>
<p>✅ PROS: <br  />
</p><ul>
<li><b>Reduces Q-function overestimation:</b> Uses two Q-networks to improve accuracy. <br  />
</li>
<li><b>Better stability than DDPG:</b> By delaying policy updates, it prevents excessive variance. <br  />
</li>
<li><b>Efficient in sample usage:</b> Unlike PPO, it is off-policy, making training faster. <br  />
</li>
<li>Works well in continuous action spaces. <br  />
</li>
</ul>
<p>❌ CONS: <br  />
</p><ul>
<li><b>Hard to tune hyperparameters:</b> Learning rate, target policy smoothing, and delay factors need fine-tuning. <br  />
</li>
<li><b>Still deterministic:</b> Unlike SAC, it doesn't naturally encourage exploration. <br  />
</li>
<li>More complex than PPO. <br  />
</li>
</ul>
<h3><a class="anchor" id="autotoc_md10"></a>
4. Deep Q-Network (DQN)</h3>
<p>✅ PROS: <br  />
</p><ul>
<li><b>Simple to implement:</b> Works well with discrete action spaces. <br  />
</li>
<li><b>Sample efficient:</b> Uses experience replay and target networks. <br  />
</li>
<li>Good for learning basic driving behaviors. <br  />
</li>
<li>Less computationally expensive compared to PPO, SAC, or TD3. <br  />
</li>
</ul>
<p>❌ CONS: <br  />
</p><ul>
<li><b>Only works with discrete actions:</b> Not suitable for smooth control in autonomous driving. <br  />
</li>
<li><b>Struggles with continuous environments:</b> Needs discretization, which can lead to suboptimal behavior. <br  />
</li>
<li><b>Prone to overestimation bias:</b> Can make learning unstable. <br  />
</li>
<li>Less effective for high-dimensional tasks. <br  />
</li>
</ul>
<h1><a class="anchor" id="autotoc_md11"></a>
Decision</h1>
<p>After taking all of the above aspects into consideration we decided that the <em><b>Proximal Policy Optimization (PPO)</b></em> would be our way to go regarding the method to develop our autonomous driving algorithm. Because our objective is to use camera frames as input to the model we concluded that this approach was the one that would perform better once fully developed due to it's stability and adaptability to different environments.</p>
<h1><a class="anchor" id="autotoc_md12"></a>
Consequences</h1>
<p>We hope that this decision simplifies the development of the model and helps us reach our expected goals. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.13.2 </li>
  </ul>
</div>
</body>
</html>
